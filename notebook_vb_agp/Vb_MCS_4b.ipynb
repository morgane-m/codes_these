{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook : Vb-AGP + MCS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'openturns'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-91c3702a6e42>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mscipy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msci\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstats\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mopenturns\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgaussian_process\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGaussianProcess\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstats\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'openturns'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy as sci\n",
    "from scipy.stats import norm\n",
    "import openturns as ot\n",
    "from gaussian_process import GaussianProcess\n",
    "from scipy import stats\n",
    "#from AK_MCS_classic import AK_MCS\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import mpl_toolkits.mplot3d\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import rcParams\n",
    "from matplotlib import rc\n",
    "\n",
    "from matplotlib import cm\n",
    "\n",
    "params = {'backend': 'ps',\n",
    "      'axes.labelsize': 55,\n",
    "      'legend.fontsize': 44,\n",
    "      'xtick.labelsize': 40,\n",
    "      'ytick.labelsize': 40,\n",
    "      'text.usetex': True,\n",
    "      'text.latex.unicode':True}\n",
    "\n",
    "rcParams.update(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cas test : Système en série à 4 branches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Illustration du cas test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f4b_mesh(x,y):\n",
    "    U1=3.+0.1*((x[:]-y[:])**2)-((x[:]+y[:])/np.sqrt(2))\n",
    "    U2=3.+0.1*((x[:]-y[:])**2)+((x[:]+y[:])/np.sqrt(2))\n",
    "     \n",
    "    U3=x[:]-y[:]+(6/np.sqrt(2))\n",
    "    U4=y[:]-x[:]+(6/np.sqrt(2))\n",
    "#    U5=X[:,0]-X[:,1]+(7/np.sqrt(2))*np.ones(n).reshape(n,1)\n",
    "#    U6=X[:,1]-X[:,0]+(7/np.sqrt(2))*np.ones(n).reshape(n,1)\n",
    "    Z=np.minimum(U1,U2)\n",
    "    Z=np.minimum(Z,np.array(U3))\n",
    "    Z=np.minimum(Z,np.array(U4))\n",
    "#    Z=np.minimum(Z,np.array(U5))\n",
    "#    Z=np.minimum(Z,np.array(U6))\n",
    "    return Z "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta=0.1\n",
    "x = np.arange(-6.0,6.0, delta)\n",
    "y = np.arange(-6.0, 6.0, delta)\n",
    "Xm, Ym = np.meshgrid(x, y)\n",
    "mshape=Xm.shape[0]\n",
    "Xmf=Xm.flatten()\n",
    "Ymf=Ym.flatten()\n",
    "Z=f4b_mesh(Xmf,Ymf)    \n",
    "Zm=Z.reshape(mshape,mshape)\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(14,8))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.contourf(Xm, Ym, Zm,levels=[-10.,0.,100.],\n",
    "             colors=('r','g'),alpha=0.05)\n",
    "CS=ax.contour(Xm, Ym, Zm,levels=[0.],linestyles= 'dashed',\n",
    "             colors='k')\n",
    "CS.collections[0].set_label(r'$G(x)=0$')\n",
    "ax.set_xlim([-6.,6.])\n",
    "ax.set_ylim([-6.0,6.0]) \n",
    "ax.set_xlabel(r'$x_1$')\n",
    "ax.set_ylabel(r'$x_2$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_4b(X):\n",
    "    U1=3+0.1*((X[0]-X[1])**2)-((X[0]+X[1])/np.sqrt(2))\n",
    "    U2=3+0.1*((X[0]-X[1])**2)+((X[0]+X[1])/np.sqrt(2))\n",
    "     \n",
    "    U3=X[0]-X[1]+(6./np.sqrt(2))\n",
    "    U4=X[1]-X[0]+(6./np.sqrt(2))\n",
    "#    U5=X[:,0]-X[:,1]+(7/np.sqrt(2))*np.ones(n).reshape(n,1)\n",
    "#    U6=X[:,1]-X[:,0]+(7/np.sqrt(2))*np.ones(n).reshape(n,1)\n",
    "    Z=np.minimum(U1,U2)\n",
    "    Z=np.minimum(Z,np.array(U3))\n",
    "    Z=np.minimum(Z,np.array(U4))\n",
    "#    Z=np.minimum(Z,np.array(U5))\n",
    "#    Z=np.minimum(Z,np.array(U6))\n",
    "    return Z \n",
    "stochastic_dim = 2\n",
    "distribution_4b=ot.Normal(ot.Point([0]*stochastic_dim), ot.CovarianceMatrix(stochastic_dim))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code algorithme Vb-AGP + MCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def Vb_AGP(performance_function,rv_distribution,initial_MC_sample_size = None,initial_doe_size = None, iter_max = 100, learning_function = 'U',cov_max = 0.1, hot_start = False,path='',corr='matern52'):\n",
    "    var_G_E_X_l=list()\n",
    "    var_X_E_G_l=list()\n",
    "    var_tot_l=list()\n",
    "    var_tot=0.\n",
    "    var_tot_inf=0.\n",
    "    var_tot_sup=0.\n",
    "\n",
    "    \n",
    "    if hot_start == False: # new initial DoE, new initial MC\n",
    "        #initial MC population\n",
    "        MC_sample_size = initial_MC_sample_size\n",
    "        MC_sample = np.array(rv_distribution.getSample(MC_sample_size))\n",
    "        # initial DoE generation\n",
    "        lhs = ot.LHSExperiment(rv_distribution, initial_doe_size)\n",
    "        lhs.setAlwaysShuffle(True) # randomized\n",
    "        spaceFilling = ot.SpaceFillingC2()\n",
    "        N = 50000\n",
    "        optimalLHSAlgorithm = ot.MonteCarloLHS(lhs, N, spaceFilling)\n",
    "        DoE=np.array(optimalLHSAlgorithm.generate())\n",
    "        DoE_size = DoE.shape[0]\n",
    "        # inital DoE evaluation \n",
    "        print(\"--------------------------------------\")\n",
    "        print(\"Evalulation of the perfomance function on the initial doe of size \",initial_doe_size)\n",
    "        print(\"--------------------------------------\")\n",
    "       # Evaluation of the performance function\n",
    "        Y = np.zeros((initial_doe_size,1))\n",
    "        i = 0\n",
    "        for x in DoE:\n",
    "            Y[i,0] = performance_function(x)\n",
    "            print(Y[i,0])\n",
    "            i = i+1\n",
    "        print( \"--------------------------------------\")\n",
    "        print(\"Saving the initial DoE \")\n",
    "        print( \"--------------------------------------\")\n",
    "        MC_sample.dump(path+str('MC_sample_init.pk'))\n",
    "        DoE.dump(path+str('DoE_init.pk'))\n",
    "        Y.dump(path+str('Y_init.pk'))\n",
    "        \n",
    "    elif hot_start == 'init': # reuse of the previous initial MC population and inital DoE\n",
    "        MC_sample = np.load(path+str('MC_sample_init.pk'),allow_pickle=True)\n",
    "        MC_sample_size = MC_sample.shape[0] \n",
    "        initial_MC_sample_size =MC_sample_size \n",
    "        DoE = np.load(path+str('DoE_init.pk'),allow_pickle=True)\n",
    "        DoE_size = DoE.shape[0]\n",
    "        Y = np.load(path+str('Y_init.pk'),allow_pickle=True)\n",
    "        print( \"--------------------------------------\")\n",
    "        print (\"Reading the initial DoE of size \",DoE_size)\n",
    "        print( \"--------------------------------------\")\n",
    " \n",
    "\n",
    "    elif hot_start == 'init_doe': # new MC populaton and reuse of the initial DoE\n",
    "        MC_sample_size = initial_MC_sample_size\n",
    "        MC_sample = np.array(rv_distribution.getSample(MC_sample_size))\n",
    "        initial_MC_sample_size =MC_sample_size \n",
    "        DoE = np.load(path+str('DoE_init.pk'),allow_pickle=True)\n",
    "        DoE_size = DoE.shape[0]\n",
    "        Y = np.zeros((DoE_size,1))\n",
    "        i = 0\n",
    "        for x in DoE:\n",
    "            Y[i,0] = performance_function(x)\n",
    "            print(Y[i,0])\n",
    "            i = i+1\n",
    "        print( \"--------------------------------------\")\n",
    "        print (\"Reading the initial DoE of size \",DoE_size)\n",
    "        print( \"--------------------------------------\")\n",
    "\n",
    "        \n",
    "        \n",
    "    elif hot_start == True: # reuse of the MC population and DoE (last update)\n",
    "        MC_sample = np.load(path+str('MC_sample.pk'),allow_pickle=True)\n",
    "        MC_sample_size = MC_sample.shape[0]  \n",
    "        initial_MC_sample_size =MC_sample_size \n",
    "        DoE = np.load(path+str('DoE.pk'),allow_pickle=True)\n",
    "        DoE_size = DoE.shape[0]\n",
    "        Y = np.load(path+str('Y.pk'),allow_pickle=True)        \n",
    "        print( \"--------------------------------------\")\n",
    "        print (\"Hot start with a DoE of size \",DoE_size)\n",
    "        print( \"--------------------------------------\")\n",
    "        \n",
    "    #construction of the GP surrogate\n",
    "    stochastic_dim = DoE.shape[1]    \n",
    "    theta0 = np.array([0.1]*stochastic_dim)\n",
    "    thetaL = np.array([1e-6]*stochastic_dim)\n",
    "    thetaU = np.array([100.0]*stochastic_dim)\n",
    "    gp_g = GaussianProcess(corr=corr,theta0 = theta0,thetaL=thetaL,thetaU=thetaU)     \n",
    "    #Vb-AGP loop\n",
    "    n_iter = 0\n",
    "    convergence = False\n",
    "\n",
    "    while n_iter<iter_max and convergence == False:\n",
    "        #fitting the GP surrogate\n",
    "        gp_g.fit(DoE,Y)\n",
    "        #estimation of the performance function surrogate model at the MC points\n",
    "        MC_sample_G,MSE = gp_g.predict(MC_sample,eval_MSE=True)\n",
    "\n",
    "        #estimation of the probability of failure Pf\n",
    "        indicatrice = np.zeros((MC_sample_size,1))\n",
    "        ind = MC_sample_G<=0.0\n",
    "        indicatrice[ind] = 1.0\n",
    "        Pf = indicatrice.sum()/MC_sample_size\n",
    "        \n",
    "        print( \"--------------------------------------\")\n",
    "        print (\"iteration number \",n_iter)\n",
    "        print (\"Pf = \",Pf)\n",
    "        \n",
    "        if Pf > 0. :    \n",
    "            # using the function sensitivity_analysis : computation of variances var_G_E_X (GP) et var_X_E_G (MC) \n",
    "            # +  control of the stopping criterion 1st step (COV_red) : end_learning = boolean \n",
    "            var_X_E_G, var_G_E_X,end_learning,var_X_E_G_inf,var_X_E_G_sup,var_G_E_X_inf,var_G_E_X_sup=sensitivity_analysis(gp_g,rv_distribution,MC_sample,Pf,cov_max)\n",
    "            var_G_E_X_l.append(var_G_E_X)\n",
    "            var_X_E_G_l.append(var_X_E_G)\n",
    "            np.array(var_X_E_G_l).dump('var_X_E_G.pk')   \n",
    "            np.array(var_G_E_X_l).dump('var_G_E_X.pk')   \n",
    "\n",
    "            print('cov_MC = ', np.sqrt(var_X_E_G)/Pf)\n",
    "            print('cov_GP = ',np.sqrt(var_G_E_X)/Pf)\n",
    "\n",
    "\n",
    "            # check of COV_tot if 1st stopping criterion verified (COV_red)\n",
    "            if end_learning : \n",
    "                # computation of the total variance : var_tot\n",
    "                print('Computation of the total COV')\n",
    "                var_tot,var_tot_inf,var_tot_sup,Pf=total_variance_bootstrap(gp_g,rv_distribution,MC_sample_size,Pf,cov_max,MC_sample)\n",
    "                var_tot_l.append(var_tot)\n",
    "                np.array(var_tot_l).dump('var_tot.pk')\n",
    "                var_target=(cov_max*Pf)**2 # limit value of the variance (corresponding to cov_max)\n",
    "                print('cov_tot = ',np.sqrt(var_tot)/Pf)\n",
    "                # check of the learning stopping criterion \n",
    "                if var_tot_sup < var_target :\n",
    "                    convergence = True \n",
    "                else : \n",
    "                    end_learning = False\n",
    "                    \n",
    "            # if stopping criterion not completed                              \n",
    "            if end_learning == False :\n",
    "                test_var = var_X_E_G > var_G_E_X\n",
    "                if test_var : # MC improvement\n",
    "                    print('higher variance due to : MC')\n",
    "                    print( \"NEW MC SAMPLE\")\n",
    "                    #adding points to the MC population\n",
    "                    new_MC_sample = np.zeros((MC_sample.shape[0]+initial_MC_sample_size,MC_sample.shape[1]))\n",
    "                    new_MC_sample[0:MC_sample.shape[0],:] = MC_sample\n",
    "                    new_MC_sample[MC_sample.shape[0]:,:] = np.array(rv_distribution.getSample(initial_MC_sample_size))\n",
    "                    MC_sample = new_MC_sample.copy()\n",
    "                    MC_sample_size = MC_sample_size + initial_MC_sample_size\n",
    "                    \n",
    "                    \n",
    "                else : # GP improvment \n",
    "                    print('higher variance due to : GP')\n",
    "                    # search of the best learning point x_star among the MC population using the EFF criterion \n",
    "                    epsilon = 2.0*np.sqrt(MSE)\n",
    "                    X = MC_sample_G[:,0]/np.sqrt(MSE)\n",
    "                    X1 = (-epsilon-MC_sample_G[:,0])/np.sqrt(MSE)\n",
    "                    X2 = (epsilon-MC_sample_G[:,0])/np.sqrt(MSE)\n",
    "                    EFF = MC_sample_G[:,0]*(2.0*sci.stats.norm.cdf(-X)-sci.stats.norm.cdf(X1)-sci.stats.norm.cdf(X2))-\\\n",
    "                    np.sqrt(MSE)*(2.0*sci.stats.norm.pdf(-X)-sci.stats.norm.pdf(X1)-sci.stats.norm.pdf(X2))+\\\n",
    "                    epsilon*(sci.stats.norm.cdf(X2)-sci.stats.norm.cdf(X1))\n",
    "                    crit_dist = False\n",
    "                    while crit_dist == False:\n",
    "                        ind_max = np.argmax(EFF)\n",
    "                        val_star = EFF[ind_max]\n",
    "                        x_star = MC_sample[ind_max]\n",
    "                        #x_star not in  DoE ?\n",
    "                        if np.sqrt(np.sum((DoE-x_star)**2,axis=1)).min()>=np.sqrt(np.sum((DoE-x_star)**2,axis=1)).max()/10**3:\n",
    "                            crit_dist = True\n",
    "                        else:\n",
    "                            crit_dist = False\n",
    "                            EFF[ind_max] = -10**6\n",
    "                    \n",
    "                    print (\"x min =\",x_star)\n",
    "                    print (\"val min=\", val_star)\n",
    "                    y_star = performance_function(x_star)\n",
    "                    new_DoE = np.zeros((DoE_size+1,DoE.shape[1]))\n",
    "                    new_DoE[0:-1,:] = DoE\n",
    "                    new_DoE[-1,:] = x_star\n",
    "                    new_Y = np.zeros((DoE_size+1,1))\n",
    "                    new_Y[0:-1,:] = Y\n",
    "                    new_Y[-1,:] = y_star\n",
    "                    DoE = new_DoE.copy()\n",
    "                    Y = new_Y.copy()\n",
    "                    DoE_size = DoE_size + 1\n",
    "                    \n",
    "      \n",
    "        if Pf == 0. :\n",
    "            # if no failure points : improvement of the GP or MC population enrichment \n",
    "            print('no failure point in MC')\n",
    "            if n_iter<4 or n_iter >10: \n",
    "                print('GP improvement')\n",
    "                epsilon = 2.0*np.sqrt(MSE)\n",
    "                X = MC_sample_G[:,0]/np.sqrt(MSE)\n",
    "                X1 = (-epsilon-MC_sample_G[:,0])/np.sqrt(MSE)\n",
    "                X2 = (epsilon-MC_sample_G[:,0])/np.sqrt(MSE)\n",
    "                EFF = MC_sample_G[:,0]*(2.0*sci.stats.norm.cdf(-X)-sci.stats.norm.cdf(X1)-sci.stats.norm.cdf(X2))-\\\n",
    "                np.sqrt(MSE)*(2.0*sci.stats.norm.pdf(-X)-sci.stats.norm.pdf(X1)-sci.stats.norm.pdf(X2))+\\\n",
    "                epsilon*(sci.stats.norm.cdf(X2)-sci.stats.norm.cdf(X1))\n",
    "                crit_dist = False\n",
    "                while crit_dist == False:\n",
    "                    ind_max = np.argmax(EFF)\n",
    "                    val_star = EFF[ind_max]\n",
    "                    x_star = MC_sample[ind_max]\n",
    "                    #x_star not in  DoE ?\n",
    "                    if np.sqrt(np.sum((DoE-x_star)**2,axis=1)).min()>=np.sqrt(np.sum((DoE-x_star)**2,axis=1)).max()/10**3:\n",
    "                        crit_dist = True\n",
    "                    else:\n",
    "                        crit_dist = False\n",
    "                        EFF[ind_max] = -10**6                \n",
    "                print (\"x min =\",x_star)\n",
    "                print (\"val min=\", val_star)\n",
    "                y_star = performance_function(x_star)\n",
    "                new_DoE = np.zeros((DoE_size+1,DoE.shape[1]))\n",
    "                new_DoE[0:-1,:] = DoE\n",
    "                new_DoE[-1,:] = x_star\n",
    "                new_Y = np.zeros((DoE_size+1,1))\n",
    "                new_Y[0:-1,:] = Y\n",
    "                new_Y[-1,:] = y_star\n",
    "                DoE = new_DoE.copy()\n",
    "                Y = new_Y.copy()\n",
    "                DoE_size = DoE_size + 1\n",
    "                \n",
    "                \n",
    "            else :     \n",
    "                print( \"NEW MC SAMPLE\")\n",
    "                #add  MC samples\n",
    "                new_MC_sample = np.zeros((MC_sample.shape[0]+initial_MC_sample_size,MC_sample.shape[1]))\n",
    "                new_MC_sample[0:MC_sample.shape[0],:] = MC_sample\n",
    "                new_MC_sample[MC_sample.shape[0]:,:] = np.array(rv_distribution.getSample(initial_MC_sample_size))\n",
    "                MC_sample = new_MC_sample.copy()\n",
    "                MC_sample_size = MC_sample_size + initial_MC_sample_size\n",
    "            \n",
    "        n_iter = n_iter + 1\n",
    "        MC_sample.dump(path+str('MC_sample.pk'))\n",
    "        DoE.dump(path+str('DoE.pk'))\n",
    "        Y.dump(path+str('Y.pk'))\n",
    "     \n",
    "    print('END Vb_AGP')\n",
    "    return Pf,DoE,n_iter,gp_g,Y,MC_sample_size,np.sqrt(var_tot)/Pf,np.sqrt(var_tot_inf)/Pf,np.sqrt(var_tot_sup)/Pf\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code : analyse de sensibilité pour l'estimation de $V_{\\tilde{X}}$ et $V_{\\mathcal{G}_n}$ dans Vb-AGP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sensitivity_analysis(gp_g,distribution,MC_sample,pf,cov_max):\n",
    "    \n",
    "    Omega=np.array([[-4.,4.]*stochastic_dim]).reshape(stochastic_dim,2) # domain for the discretization for KL \n",
    "    eig, vect_star, nodes, weights, M =  gp_g.K_eig_noncond_randuniform(nb_vect=1000, Omega=Omega,normalize_Omega=False,n_start=2000,eps=1e-8)\n",
    "    phi = gp_g.eval_phi_KL( weights, nodes, eig, vect_star, M, MC_sample) #evaluation of the eigenfunctions at the MC points\n",
    "    # setup for GP trajectories simulation for V_G_E_X\n",
    "    n_traj_max=15000 \n",
    "    n_traj=2000\n",
    "    \n",
    "    pf_samples_list=list()\n",
    "    phi_n = stats.norm()\n",
    "    cond = False\n",
    "    end_learning = False\n",
    "    \n",
    "    ### Computation of var_X_E_G\n",
    "    MC_sample_size=MC_sample.shape[0]\n",
    "    MC_sample_G,MSE=gp_g.predict(MC_sample,eval_MSE=True)\n",
    "    U=-MC_sample_G[:,0]/np.sqrt(MSE)\n",
    "    # computation of p_i\n",
    "    p=phi_n.cdf(U.reshape((MC_sample_size,1)))\n",
    "    var_X_E_G=np.var(p)/MC_sample_size\n",
    "    # computation of var_X_E_G confidence interval \n",
    "    n=MC_sample_size\n",
    "    p_mean=p.mean()\n",
    "    v=var_X_E_G\n",
    "    v_k=np.var((p-p_mean)**2)/(n*(n-1)**2)\n",
    "    var_X_E_G_inf=v-3*np.sqrt(v_k)\n",
    "    var_X_E_G_sup=v+3*np.sqrt(v_k)\n",
    "    \n",
    "    ### Computation of var_G_E_X\n",
    "    cond =False\n",
    "    pf_samples=np.array([0])\n",
    "    while not(cond and pf_samples.shape[0] > 3000 or (pf_samples.shape[0] > n_traj_max) and pf_samples.shape[0] > 3000) :  \n",
    "        # simu trajectoires\n",
    "        xi_traj=np.random.normal(0,1,(M,n_traj))\n",
    "        traj,phi=gp_g.eval_KL_expansion_trajectories_phi(weights=weights,nodes=nodes,eig=eig,vect=vect_star, M=M, x_new=MC_sample,xi_traj=xi_traj,phi=phi)\n",
    "        # calcul des Pf par traj \n",
    "        indicatrice = np.zeros((MC_sample_size,n_traj))\n",
    "        ind = traj.T <=0.0\n",
    "        indicatrice[ind] = 1.0\n",
    "        pf_mu= indicatrice.sum(axis=0)/MC_sample_size\n",
    "        pf_samples=pf_mu\n",
    "        pf_samples_list.append(pf_samples)\n",
    "        pf_samples=np.array(pf_samples_list).flatten()\n",
    "        pf_tot_mean=np.mean(pf_samples)\n",
    "\n",
    "        var_G_E_X=((pf_samples-pf_tot_mean)**2).sum()/(pf_samples.shape[0]-1)  \n",
    "        print('n_traj :',pf_samples.shape[0])\n",
    "        # computation of var_G_E_X confidence interval \n",
    "        n=pf_samples.shape[0]\n",
    "        v=var_G_E_X\n",
    "        v_k=np.var((pf_samples-pf_tot_mean)**2)*n/(n-1)**2\n",
    "        var_G_E_X_inf=v-3*np.sqrt(v_k)\n",
    "        var_G_E_X_sup=v+3*np.sqrt(v_k)\n",
    "        # verification that the intersection of the two confidence intervals is null \n",
    "        if not((var_G_E_X_inf < var_X_E_G_inf < var_G_E_X_sup) or (var_G_E_X_inf < var_X_E_G_sup < var_G_E_X_sup)):\n",
    "            cond= True\n",
    "            \n",
    "    # verification of the 1st stopping criterion (COV_red)\n",
    "    var_target=(cov_max*pf)**2\n",
    "    if (var_G_E_X_sup+ var_X_E_G_sup < var_target):\n",
    "        print('COV_red = ',np.sqrt(var_G_E_X+ var_X_E_G)/pf)\n",
    "        end_learning=True      \n",
    "     \n",
    "    \n",
    "    return var_X_E_G, var_G_E_X,end_learning,var_X_E_G_inf,var_X_E_G_sup,var_G_E_X_inf,var_G_E_X_sup\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code : Estimation de $V_{tot}$ et $\\hat{P}_f^t$ par Bootstrap "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_variance_bootstrap(gp_g,distribution,MC_sample_size,pf,cov_max,MC_sample):\n",
    "    stochastic_dim=gp_g.X.shape[1]\n",
    "    Omega=np.array([[-4.,4.]*stochastic_dim]).reshape(stochastic_dim,2)\n",
    "    eig, vect_star, nodes, weights, M =  gp_g.K_eig_noncond_randuniform(nb_vect=1000, Omega=Omega,normalize_Omega=False,n_start=2000,eps=1e-8)\n",
    "    phi = gp_g.eval_phi_KL( weights, nodes, eig, vect_star, M, MC_sample)\n",
    "    # number of realizations of MC and GP \n",
    "    N_mc_i=500 \n",
    "    N_mc_max=2000 \n",
    "    pf_list=list()\n",
    "    var_target=(cov_max*pf)**2\n",
    "    var_tot_inf=var_target-1\n",
    "    var_tot_sup=var_target+1\n",
    "    N_mc=N_mc_i\n",
    "    traj_ind=np.arange(MC_sample_size)\n",
    "    while not( not(var_tot_inf<var_target<var_tot_sup) or N_mc>N_mc_max) :\n",
    "        # simulation of n_traj on the MC population \n",
    "        n_traj=N_mc_i\n",
    "        xi_traj=np.random.normal(0,1,(M,n_traj))\n",
    "        traj_i,phi=gp_g.eval_KL_expansion_trajectories_phi(weights=weights,nodes=nodes,eig=eig,vect=vect_star, M=M, x_new=MC_sample,xi_traj=xi_traj,phi=phi)\n",
    "        # Boostrap to simulate different MC population \n",
    "        for i in np.arange(N_mc_i):\n",
    "            ind_boot=np.random.choice(traj_ind,size=MC_sample_size,replace=True)\n",
    "            traj=traj_i.T[ind_boot,i]\n",
    "            indicatrice = np.zeros((MC_sample_size,n_traj))\n",
    "            ind = traj <=0.0\n",
    "            indicatrice[ind] = 1.0\n",
    "            indicatrice=indicatrice\n",
    "            Pf = indicatrice.sum(axis=0)/MC_sample_size\n",
    "            pf_list.append(Pf)\n",
    "        \n",
    "        pf=np.array(pf_list).flatten()\n",
    "        pf_tot_mean=np.mean(pf)\n",
    "        var_tot=((pf-pf_tot_mean)**2).sum()/(pf.shape[0]-1)     \n",
    "        n=pf.shape[0]\n",
    "        v=var_tot\n",
    "        v_k=np.var((pf-pf_tot_mean)**2)*n/(n-1)**2\n",
    "        var_tot_inf=v-3*np.sqrt(v_k)\n",
    "        var_tot_sup=v+3*np.sqrt(v_k)\n",
    "        N_mc=N_mc+N_mc_i \n",
    "\n",
    "    return var_tot,var_tot_inf,var_tot_sup,pf_tot_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choix des paramètres de l'algorithme "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########  choix cas test + distribution et param algo \n",
    "fct_test= f_4b\n",
    "distrib_test=distribution_4b\n",
    "initial_doe_size=16\n",
    "initial_MC_sample_size=5000\n",
    "cov_max=0.1\n",
    "iter_max = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pf,DoE,n_iter,gp_g, Y,MC_sample_size,cov_tot, cov_tot_inf,cov_tot_sup= Vb_AGP(fct_test,distrib_test,initial_MC_sample_size = initial_MC_sample_size,initial_doe_size = initial_doe_size,  iter_max = iter_max, cov_max=cov_max,corr='matern52')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Pf = ',Pf)\n",
    "print('COV total = ', cov_tot)\n",
    "print('DoE size = ', int(DoE.shape[0]))\n",
    "print('MC size = ',MC_sample_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
