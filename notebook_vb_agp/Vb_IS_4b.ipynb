{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Vb-AGP + IS : 4 branches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sci\n",
    "from scipy.stats import norm\n",
    "import openturns as ot\n",
    "from gaussian_process import GaussianProcess\n",
    "from scipy import stats\n",
    "from NAIS_min2_ot import NAIS_m\n",
    "import math\n",
    "#from AK_MCS_classic import AK_MCS\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import mpl_toolkits.mplot3d\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import rcParams\n",
    "from matplotlib import rc\n",
    "\n",
    "from matplotlib import cm\n",
    "\n",
    "params = {'backend': 'ps',\n",
    "      'axes.labelsize': 55,\n",
    "      'legend.fontsize': 44,\n",
    "      'xtick.labelsize': 40,\n",
    "      'ytick.labelsize': 40,\n",
    "      'text.usetex': True,\n",
    "      'text.latex.unicode':True}\n",
    "\n",
    "rcParams.update(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code algorithme Vb-AGP + MCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Vb_AGP_IS(performance_function,rv_distribution,initial_MC_sample_size = None,initial_doe_size = None, iter_max = 100, cov_max = 0.1, hot_start = False,path='',max_IS_size=50000,corr='matern52'):\n",
    "    var_G_E_X_l=list()\n",
    "    var_X_E_G_l=list()\n",
    "    var_tot_l=list()\n",
    "    var_tot=0.\n",
    "    var_tot_inf=0.\n",
    "    var_tot_sup=0.\n",
    "    pf_l=list()\n",
    "    gp_new=True\n",
    "    if hot_start == False:\n",
    "        #initial MC population\n",
    "        MC_sample_size = initial_MC_sample_size\n",
    "        MC_sample = np.array(rv_distribution.getSample(MC_sample_size))\n",
    "        # initial DoE generation\n",
    "        lhs = ot.LHSExperiment(rv_distribution, initial_doe_size)\n",
    "        lhs.setAlwaysShuffle(True) # randomized\n",
    "        spaceFilling = ot.SpaceFillingC2()\n",
    "        N = 50000\n",
    "        optimalLHSAlgorithm = ot.MonteCarloLHS(lhs, N, spaceFilling)\n",
    "        DoE=np.array(optimalLHSAlgorithm.generate())\n",
    "        \n",
    "        DoE_size = DoE.shape[0]\n",
    "        print(\"--------------------------------------\")\n",
    "        print(\"Evalulation of the perfomance function on the initial doe of size \",initial_doe_size)\n",
    "        print(\"--------------------------------------\")\n",
    "#        Evaluation of the performance function\n",
    "        Y = np.zeros((initial_doe_size,1))\n",
    "        i = 0\n",
    "        for x in DoE:\n",
    "            Y[i,0] = performance_function(x)\n",
    "            print(Y[i,0])\n",
    "            i = i+1\n",
    "        print( \"--------------------------------------\")\n",
    "        print(\"Saving the initial DoE \")\n",
    "        print( \"--------------------------------------\")\n",
    "        MC_sample.dump(path+str('MC_sample_init.pk'))\n",
    "        DoE.dump(path+str('DoE_init.pk'))\n",
    "        Y.dump(path+str('Y_init.pk'))\n",
    "        \n",
    "    elif hot_start == 'init':\n",
    "        MC_sample = np.load(path+str('MC_sample_init.pk'))\n",
    "        MC_sample_size = MC_sample.shape[0] \n",
    "        initial_MC_sample_size =MC_sample_size \n",
    "        DoE = np.load(path+str('DoE_init.pk'))\n",
    "        DoE_size = DoE.shape[0]\n",
    "        Y = np.load(path+str('Y_init.pk'))\n",
    "        print( \"--------------------------------------\")\n",
    "        print (\"Reading the initial DoE of size \",DoE_size)\n",
    "        print( \"--------------------------------------\")\n",
    " \n",
    "\n",
    "    elif hot_start == 'init_doe':\n",
    "        MC_sample_size = initial_MC_sample_size\n",
    "        MC_sample = rv_distribution.rvs(MC_sample_size)\n",
    "    \n",
    "        DoE = np.load(path+str('DoE_init.pk'),allow_pickle=True)\n",
    "        DoE_size = DoE.shape[0]\n",
    "        Y = np.zeros((DoE_size,1))\n",
    "        i = 0\n",
    "        for x in DoE:\n",
    "            Y[i,0] = performance_function(x)\n",
    "            print(Y[i,0])\n",
    "            i = i+1\n",
    "        print( \"--------------------------------------\")\n",
    "        print (\"Reading the initial DoE of size \",DoE_size)\n",
    "        print( \"--------------------------------------\")\n",
    "\n",
    "        \n",
    "        \n",
    "    elif hot_start == True:\n",
    "        MC_sample = np.load(path+str('MC_sample.pk'),allow_pickle=True)\n",
    "        MC_sample_size = MC_sample.shape[0]  \n",
    "        initial_MC_sample_size =MC_sample_size \n",
    "        DoE = np.load(path+str('DoE.pk'),allow_pickle=True)\n",
    "        DoE_size = DoE.shape[0]\n",
    "        Y = np.load(path+str('Y.pk'),allow_pickle=True)\n",
    "        print( \"--------------------------------------\")\n",
    "        print (\"Hot start with a DoE of size \",DoE_size)\n",
    "        print( \"--------------------------------------\")\n",
    "        \n",
    "    #construction of the GP surrogate\n",
    "    stochastic_dim = DoE.shape[1]    \n",
    "    theta0 = np.array([0.1]*stochastic_dim)\n",
    "    thetaL = np.array([1e-6]*stochastic_dim)\n",
    "    thetaU = np.array([100.0]*stochastic_dim)\n",
    "    gp_g = GaussianProcess(corr=corr,theta0 = theta0,thetaL=thetaL,thetaU=thetaU)     \n",
    "\n",
    "\n",
    "    #AK-MCS loop\n",
    "    n_iter = 0\n",
    "    convergence = False\n",
    "    learn_Faux=True \n",
    "    MC_sample_IS=MC_sample\n",
    "    sample_IS_tot=MC_sample\n",
    "    IS_sample_size=MC_sample_size\n",
    "    initial_IS_sample_size=10000\n",
    "    f_aux=False\n",
    "    weights_IS=np.ones((IS_sample_size,1))\n",
    "\n",
    "\n",
    "    while n_iter<iter_max and convergence == False:\n",
    "        #fitting the GP surrogate\n",
    "        gp_g.fit(DoE,Y)\n",
    "        #estimation of the performance function surrogate model at the MC points\n",
    "        MC_sample_G_IS = gp_g.predict(MC_sample_IS)\n",
    "        \n",
    "\n",
    "\n",
    "        \n",
    "        #estimation of Pf on IS samples\n",
    "        indicatrice = np.zeros((IS_sample_size,1))\n",
    "        ind = MC_sample_G_IS<=0.0\n",
    "        indicatrice[ind] = 1.0\n",
    "        indicatrice=indicatrice*weights_IS\n",
    "        Pf = indicatrice.sum()/IS_sample_size\n",
    "        \n",
    "        print( \"--------------------------------------\")\n",
    "        print (\"iteration number \",n_iter)\n",
    "        print (\"Pf = \",Pf)\n",
    "        \n",
    "        if Pf > 0. :\n",
    "                \n",
    "            # using the function sensitivity_analysis : computation of variances var_G_E_X (GP) et var_X_E_G (MC)\n",
    "            # OR the function sensitivity_analysis_IS : computation of variances var_G_E_X (GP) et var_X_E_G (IS)\n",
    "            # +  control of the stopping criterion 1st step (COV_red) : end_learning = boolean \n",
    "            if f_aux==False : # before 1st NAIS\n",
    "                results_sensitivity=sensitivity_analysis_MC(gp_g,rv_distribution,MC_sample,Pf,cov_max,gp_new=gp_new,path=path)\n",
    "                var_X_E_G, var_G_E_X,end_learning,var_X_E_G_inf,var_X_E_G_sup,var_G_E_X_inf,var_G_E_X_sup=results_sensitivity\n",
    "            else:\n",
    "                results_sensitivity=sensitivity_analysis_IS(gp_g,rv_distribution,MC_sample_IS,weights_IS,Pf,cov_max,f_aux,learn_Faux,gp_new=gp_new,path=path)\n",
    "                end_learning,var_X_E_G, var_G_E_X=results_sensitivity        \n",
    "                \n",
    "                    \n",
    "            var_G_E_X_l.append(var_G_E_X)\n",
    "            var_X_E_G_l.append(var_X_E_G)\n",
    "         \n",
    "            np.array(var_X_E_G_l).dump('var_X_E_G_IS.pk')   \n",
    "            np.array(var_G_E_X_l).dump('var_G_E_X_IS.pk')    \n",
    "            \n",
    "            print('cov_IS = ', np.sqrt(var_X_E_G)/Pf)\n",
    "            print('cov_GP = ',np.sqrt(var_G_E_X)/Pf)\n",
    "            \n",
    "           \n",
    "            \n",
    "            pf_l.append(Pf)  \n",
    "            np.array(pf_l).dump(path+'pf_algo_IS.pk')   \n",
    "\n",
    "            \n",
    "            if end_learning : \n",
    "                var_tot,var_tot_inf,var_tot_sup,Pf=total_variance_bootstrap(gp_g,rv_distribution,IS_sample_size,Pf,cov_max,MC_sample_IS,weights_IS,f_aux)\n",
    "                var_target=(cov_max*Pf)**2\n",
    "                \n",
    "                print('COV_tot =',np.sqrt(var_tot)/Pf)\n",
    "                if var_tot_sup < var_target :\n",
    "                    end_learning=True\n",
    "                else : \n",
    "                    end_learning = False\n",
    "                \n",
    "                \n",
    "                    \n",
    "\n",
    "\n",
    "            if end_learning :\n",
    "                print('End of learning phase')\n",
    "                convergence=True\n",
    "                \n",
    "                \n",
    "            # if stopping criterion not completed        \n",
    "            if end_learning == False : \n",
    "                test_var = var_X_E_G > var_G_E_X and IS_sample_size < max_IS_size\n",
    "                        \n",
    "                if test_var :\n",
    "                    print('higher variance due to : MC/IS')   \n",
    "                    if learn_Faux : \n",
    "                        gp_new=False\n",
    "                        print( \"UPDATE OF AUXILIARY DENSITY\")\n",
    "                        #NAIS \n",
    "                        IS_sample_size = 10000 # final IS population size\n",
    "                        NAIS_budget= 30000 # nb of samples per iteration\n",
    "                        Pf, f_aux, var_NAIS,sample_IS_tot =NAIS_m(20.,0.,rv_distribution,NAIS_budget,gp_g,iter_max=40)\n",
    "                        f_aux=f_aux[-1]\n",
    "                        print( \"NEW IS INITIAL POPULATION\")\n",
    "                        MC_sample_IS=f_aux.resample(IS_sample_size).T\n",
    "                        learn_Faux=False\n",
    "                        t0=t.time()\n",
    "                        print(\"WEIGHTS COMPUTATION\")\n",
    "                        f_aux_ISsamples=f_aux.pdf(MC_sample_IS.T)\n",
    "                        distribution_ISsample=np.array(rv_distribution.computePDF(MC_sample_IS))[:,0]\n",
    "                        weights_IS=distribution_ISsamples/f_aux_ISsamples\n",
    "                        weights_IS=weights_IS.reshape(weights_IS.shape[0],1)\n",
    "                        t1=t.time()\n",
    "\n",
    "                        \n",
    "                    else: \n",
    "                        gp_new=False\n",
    "                        # + grande pop IS \n",
    "                        print( \"NEW IS SAMPLES\")\n",
    "                        #add  MC samples\n",
    "                        new_MC_sample_IS = np.zeros((MC_sample_IS.shape[0]+initial_IS_sample_size,MC_sample_IS.shape[1]))\n",
    "                        new_MC_sample_IS[0:MC_sample_IS.shape[0],:] = MC_sample_IS\n",
    "                        new_MC_sample_IS[MC_sample_IS.shape[0]:,:] = f_aux.resample(initial_IS_sample_size).T\n",
    "                        print(\"WEIGHTS COMPUTATION\")\n",
    "                        new_weights_IS = np.zeros((MC_sample_IS.shape[0]+initial_IS_sample_size,1))\n",
    "                        new_weights_IS[0:weights_IS.shape[0],:] = weights_IS\n",
    "                        f_aux_ISsamples=f_aux.pdf(new_MC_sample_IS[weights_IS.shape[0]:,:].T)\n",
    "                        distribution_ISsamples=np.array(rv_distribution.computePDF(new_MC_sample_IS[weights_IS.shape[0]:,:]))[:,0]\n",
    "                        new_weights_IS[MC_sample_IS.shape[0]:,:] =(distribution_ISsamples/f_aux_ISsamples).reshape(initial_IS_sample_size,1)\n",
    "                        \n",
    "                        weights_IS=new_weights_IS.copy()\n",
    "                        MC_sample_IS = new_MC_sample_IS.copy()\n",
    "                        IS_sample_size = IS_sample_size + initial_IS_sample_size\n",
    "                        learn_Faux=False \n",
    "\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                else :\n",
    "                    gp_new=True\n",
    "                    print('higher variance due to : GP')\n",
    "                    MC_sample_G_IS_tot,MSE_IS_tot = gp_g.predict(sample_IS_tot,eval_MSE=True)\n",
    "                    MC_sample_G_MC,MSE_MC = gp_g.predict(MC_sample,eval_MSE=True)\n",
    "                    MC_sample_G=np.concatenate((MC_sample_G_IS_tot,MC_sample_G_MC))\n",
    "                    MSE=np.concatenate((MSE_IS_tot,MSE_MC))\n",
    "                    MC_sample_tot=np.concatenate((sample_IS_tot,MC_sample))\n",
    "\n",
    "        \n",
    "\n",
    "            \n",
    "                    epsilon = 2.0*np.sqrt(MSE)\n",
    "                    X = MC_sample_G[:,0]/np.sqrt(MSE)\n",
    "                    X1 = (-epsilon-MC_sample_G[:,0])/np.sqrt(MSE)\n",
    "                    X2 = (epsilon-MC_sample_G[:,0])/np.sqrt(MSE)\n",
    "                    EFF = MC_sample_G[:,0]*(2.0*sci.stats.norm.cdf(-X)-sci.stats.norm.cdf(X1)-sci.stats.norm.cdf(X2))-\\\n",
    "                    np.sqrt(MSE)*(2.0*sci.stats.norm.pdf(-X)-sci.stats.norm.pdf(X1)-sci.stats.norm.pdf(X2))+\\\n",
    "                    epsilon*(sci.stats.norm.cdf(X2)-sci.stats.norm.cdf(X1))\n",
    "                    crit_dist = False\n",
    "                    while crit_dist == False:\n",
    "                        ind_max = np.argmax(EFF)\n",
    "                        val_star = EFF[ind_max]\n",
    "                        x_star = MC_sample_tot[ind_max]\n",
    "                        #x_star not in  DoE ?\n",
    "                        if np.sqrt(np.sum((DoE-x_star)**2,axis=1)).min()>=np.sqrt(np.sum((DoE-x_star)**2,axis=1)).max()/10**3:\n",
    "                            crit_dist = True\n",
    "                        else:\n",
    "                            crit_dist = False\n",
    "                            EFF[ind_max] = -10**6\n",
    "                    \n",
    "                    print (\"x min =\",x_star)\n",
    "                    print (\"val min=\", val_star)\n",
    "                    y_star = performance_function(x_star)\n",
    "                    new_DoE = np.zeros((DoE_size+1,DoE.shape[1]))\n",
    "                    new_DoE[0:-1,:] = DoE\n",
    "                    new_DoE[-1,:] = x_star\n",
    "                    new_Y = np.zeros((DoE_size+1,1))\n",
    "                    new_Y[0:-1,:] = Y\n",
    "                    new_Y[-1,:] = y_star\n",
    "                    DoE = new_DoE.copy()\n",
    "                    Y = new_Y.copy()                    \n",
    "                    DoE_size = DoE_size + 1\n",
    "                    learn_Faux=True\n",
    "                \n",
    "\n",
    "\n",
    "                    \n",
    "        elif Pf == 0. :\n",
    "            print('no failure point in MC')\n",
    "            print( \"UPDATE OF AUXILIARY DENSITY\")\n",
    "            NAIS_budget= 30000 # samples per iteration\n",
    "            IS_sample_size = 10000 # final IS population size\n",
    "            Pf, f_aux, var_NAIS,sample_IS_tot =NAIS_m(20.,0.,rv_distribution,NAIS_budget,gp_g,iter_max=30)\n",
    "            f_aux=f_aux[-1]\n",
    "            print( \"NEW IS INITIAL POP\")\n",
    "            MC_sample_IS=f_aux.resample(IS_sample_size).T\n",
    "            learn_Faux=False\n",
    "            f_aux_ISsamples=f_aux.pdf(MC_sample_IS.T)\n",
    "            distribution_ISsamples=np.array(rv_distribution.computePDF(MC_sample_IS))[:,0]\n",
    "            weights_IS=distribution_ISsamples/f_aux_ISsamples\n",
    "            weights_IS=weights_IS.reshape(weights_IS.shape[0],1)\n",
    "\n",
    "            \n",
    "            if Pf >= 0 :\n",
    "                # add learning points for the GP \n",
    "                size = 2*stochastic_dim\n",
    "                print('Enrichment of the GP')\n",
    "                print('Number of learning points = ', size)\n",
    "                for i in np.arange(size): \n",
    "                    MC_sample_G,MSE = gp_g.predict(sample_IS_tot,eval_MSE=True)\n",
    "                    X = MC_sample_G[:,0]/np.sqrt(MSE)\n",
    "                    epsilon = 2.0*np.sqrt(MSE)\n",
    "                    X1 = (-epsilon-MC_sample_G[:,0])/np.sqrt(MSE)\n",
    "                    X2 = (epsilon-MC_sample_G[:,0])/np.sqrt(MSE)\n",
    "                    EFF = MC_sample_G[:,0]*(2.0*sci.stats.norm.cdf(-X)-sci.stats.norm.cdf(X1)-sci.stats.norm.cdf(X2))-\\\n",
    "                    np.sqrt(MSE)*(2.0*sci.stats.norm.pdf(-X)-sci.stats.norm.pdf(X1)-sci.stats.norm.pdf(X2))+\\\n",
    "                    epsilon*(sci.stats.norm.cdf(X2)-sci.stats.norm.cdf(X1))\n",
    "                    ind_max = np.argmax(EFF)\n",
    "                    val_star = EFF[ind_max]\n",
    "                    DoE_IS=sample_IS_tot[ind_max]\n",
    "                    EFF[ind_max] = -10**6\n",
    "                    x=DoE_IS\n",
    "                    y_star=performance_function(x)\n",
    "                    print(y_star)\n",
    "                    new_DoE = np.zeros((DoE_size+1,DoE.shape[1]))\n",
    "                    new_DoE[0:DoE_size,:] = DoE\n",
    "                    new_DoE[DoE_size:,:] = DoE_IS\n",
    "                    new_Y = np.zeros((DoE_size+1,1))\n",
    "                    new_Y[0:DoE_size,:] = Y\n",
    "                    new_Y[DoE_size:,:] = y_star\n",
    "                    DoE = new_DoE.copy()\n",
    "                    Y = new_Y.copy()\n",
    "                    gp_g.fit(DoE,Y)\n",
    "                    DoE_size = DoE_size + 1\n",
    "\n",
    "\n",
    "                    \n",
    "                learn_Faux=True\n",
    "\n",
    "\n",
    "\n",
    "                    \n",
    "                \n",
    "                \n",
    "        n_iter = n_iter + 1\n",
    "        MC_sample.dump(path+str('MC_sample.pk'))\n",
    "        DoE.dump(path+str('DoE.pk'))\n",
    "        Y.dump(path+str('Y.pk'))\n",
    "     \n",
    "    print('FIN Vb-AGP + IS')\n",
    "    return Pf, DoE, n_iter, gp_g, Y, IS_sample_size, f_aux,np.sqrt(var_tot)/Pf,np.sqrt(var_tot_inf)/Pf,np.sqrt(var_tot_sup)/Pf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code : analyse de sensibilité pour l'estimation de $V_{\\tilde{X}}$ et $V_{\\mathcal{G}_n}$ quand $\\tilde{X}$ est une population de MC "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sensitivity_analysis(gp_g,distribution,MC_sample,pf,cov_max):\n",
    "    \n",
    "    Omega=np.array([[-4.,4.]*stochastic_dim]).reshape(stochastic_dim,2) # domain for the discretization for KL \n",
    "    eig, vect_star, nodes, weights, M =  gp_g.K_eig_noncond_randuniform(nb_vect=1000, Omega=Omega,normalize_Omega=False,n_start=2000,eps=1e-8)\n",
    "    phi = gp_g.eval_phi_KL( weights, nodes, eig, vect_star, M, MC_sample) #evaluation of the eigenfunctions at the MC points\n",
    "    # setup for GP trajectories simulation for V_G_E_X\n",
    "    n_traj_max=15000 \n",
    "    n_traj=2000\n",
    "    \n",
    "    pf_samples_list=list()\n",
    "    phi_n = stats.norm()\n",
    "    cond = False\n",
    "    end_learning = False\n",
    "    \n",
    "    ### Computation of var_X_E_G\n",
    "    MC_sample_size=MC_sample.shape[0]\n",
    "    MC_sample_G,MSE=gp_g.predict(MC_sample,eval_MSE=True)\n",
    "    U=-MC_sample_G[:,0]/np.sqrt(MSE)\n",
    "    # computation of p_i\n",
    "    p=phi_n.cdf(U.reshape((MC_sample_size,1)))\n",
    "    var_X_E_G=np.var(p)/MC_sample_size\n",
    "    # computation of var_X_E_G confidence interval \n",
    "    n=MC_sample_size\n",
    "    p_mean=p.mean()\n",
    "    v=var_X_E_G\n",
    "    v_k=np.var((p-p_mean)**2)/(n*(n-1)**2)\n",
    "    var_X_E_G_inf=v-3*np.sqrt(v_k)\n",
    "    var_X_E_G_sup=v+3*np.sqrt(v_k)\n",
    "    \n",
    "    ### Computation of var_G_E_X\n",
    "    cond =False\n",
    "    pf_samples=np.array([0])\n",
    "    while not(cond and pf_samples.shape[0] > 3000 or (pf_samples.shape[0] > n_traj_max) and pf_samples.shape[0] > 3000) :  \n",
    "        # simu trajectoires\n",
    "        xi_traj=np.random.normal(0,1,(M,n_traj))\n",
    "        traj,phi=gp_g.eval_KL_expansion_trajectories_phi(weights=weights,nodes=nodes,eig=eig,vect=vect_star, M=M, x_new=MC_sample,xi_traj=xi_traj,phi=phi)\n",
    "        # calcul des Pf par traj \n",
    "        indicatrice = np.zeros((MC_sample_size,n_traj))\n",
    "        ind = traj.T <=0.0\n",
    "        indicatrice[ind] = 1.0\n",
    "        pf_mu= indicatrice.sum(axis=0)/MC_sample_size\n",
    "        pf_samples=pf_mu\n",
    "        pf_samples_list.append(pf_samples)\n",
    "        pf_samples=np.array(pf_samples_list).flatten()\n",
    "        pf_tot_mean=np.mean(pf_samples)\n",
    "\n",
    "        var_G_E_X=((pf_samples-pf_tot_mean)**2).sum()/(pf_samples.shape[0]-1)  \n",
    "        print('n_traj :',pf_samples.shape[0])\n",
    "        # computation of var_G_E_X confidence interval \n",
    "        n=pf_samples.shape[0]\n",
    "        v=var_G_E_X\n",
    "        v_k=np.var((pf_samples-pf_tot_mean)**2)*n/(n-1)**2\n",
    "        var_G_E_X_inf=v-3*np.sqrt(v_k)\n",
    "        var_G_E_X_sup=v+3*np.sqrt(v_k)\n",
    "        # verification that the intersection of the two confidence intervals is null \n",
    "        if not((var_G_E_X_inf < var_X_E_G_inf < var_G_E_X_sup) or (var_G_E_X_inf < var_X_E_G_sup < var_G_E_X_sup)):\n",
    "            cond= True\n",
    "            \n",
    "    # verification of the 1st stopping criterion (COV_red)\n",
    "    var_target=(cov_max*pf)**2\n",
    "    if (var_G_E_X_sup+ var_X_E_G_sup < var_target):\n",
    "        print('COV_red = ',np.sqrt(var_G_E_X+ var_X_E_G)/pf)\n",
    "        end_learning=True      \n",
    "     \n",
    "    \n",
    "    return var_X_E_G, var_G_E_X,end_learning,var_X_E_G_inf,var_X_E_G_sup,var_G_E_X_inf,var_G_E_X_sup\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code : analyse de sensibilité pour l'estimation de $V_{\\tilde{X}}$ et $V_{\\mathcal{G}_n}$ quand $\\tilde{X}$ est une population d'IS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sensitivity_analysis_IS(gp_g,distribution,IS_sample,weights_IS,pf,cov_max,f_aux,learn_Faux,gp_new=True,path=''):\n",
    "   \n",
    "    stochastic_dim=gp_g.X.shape[1]\n",
    "    if gp_new : \n",
    "        Omega=np.array([[-8.,8.]*stochastic_dim]).reshape(stochastic_dim,2)\n",
    "\n",
    "        eig, vect_star, nodes, weights, M =   gp_g.K_eig_noncond_randuniform(nb_vect=1100, Omega=Omega,n_start=2000,eps=1e-8) \n",
    "        eig.dump(path+'eig.pk')\n",
    "        vect_star[:,:M].dump(path+'vect_star.pk')\n",
    "        nodes.dump(path+'nodes.pk')\n",
    "        weights.dump(path+'weights.pk')\n",
    "        np.array(M).dump(path+'M.pk')\n",
    "    else : \n",
    "        eig=np.load(path+'eig.pk',allow_pickle=True)\n",
    "        vect_star=np.load(path+'vect_star.pk',allow_pickle=True)\n",
    "        nodes=np.load(path+'nodes.pk',allow_pickle=True)\n",
    "        weights=np.load(path+'weights.pk',allow_pickle=True)\n",
    "        M=int(np.load(path+'M.pk',allow_pickle=True))\n",
    "        \n",
    "        \n",
    "    phi = gp_g.eval_phi_KL( weights, nodes, eig, vect_star, M, IS_sample)\n",
    "\n",
    "     # setup for GP trajectories simulation for V_G_E_X\n",
    "    n_traj_max=10000\n",
    "    n_traj=4000\n",
    "    pf_samples_list = list()\n",
    "    phi_n = stats.norm()\n",
    "    end_learning = False\n",
    "    IS_sample_size=IS_sample.shape[0]\n",
    "    \n",
    "    MC_sample_G,MSE=gp_g.predict(IS_sample,eval_MSE=True)\n",
    "    U=-MC_sample_G[:,0]/np.sqrt(MSE)\n",
    "    # calcul des pi \n",
    "    p=phi_n.cdf(U.reshape((IS_sample_size,1)))\n",
    "    var_X_E_G=np.var(p*weights_IS)/IS_sample_size\n",
    "    n=IS_sample_size\n",
    "    p_mean=np.mean(p*weights_IS)\n",
    "    v=var_X_E_G\n",
    "    # var_X_E_G confidence interval estimation \n",
    "    v_k=np.var((p*weights_IS-p_mean)**2)/(n*(n-1)**2)\n",
    "    var_X_E_G_inf=v-3*np.sqrt(v_k)\n",
    "    var_X_E_G_sup=v+3*np.sqrt(v_k)\n",
    "    \n",
    "    Pf_p=np.mean(p*weights_IS)\n",
    "    cond =False\n",
    "    pf_samples=np.array([0])\n",
    "    while not(cond and pf_samples.shape[0] > 2000 or (pf_samples.shape[0] > n_traj_max) and pf_samples.shape[0] > 2000): \n",
    "        xi_traj=np.random.normal(0,1,(M,n_traj))\n",
    "        traj,phi=gp_g.eval_KL_expansion_trajectories_phi(weights=weights,nodes=nodes,eig=eig,vect=vect_star, M=M, x_new=IS_sample,xi_traj=xi_traj,phi=phi)\n",
    "        indicatrice = np.zeros((IS_sample_size,n_traj))\n",
    "        ind = traj.T <=0.0\n",
    "        indicatrice[ind] = 1.0\n",
    "        indicatrice=indicatrice*weights_IS\n",
    "        pf_mu= indicatrice.sum(axis=0)/IS_sample_size\n",
    "        pf_samples=pf_mu\n",
    "        pf_samples_list.append(pf_samples)\n",
    "        pf_samples=np.array(pf_samples_list).flatten()\n",
    "        pf_tot_mean=np.mean(pf_samples)\n",
    "\n",
    "        var_G_E_X=((pf_samples-pf_tot_mean)**2).sum()/(pf_samples.shape[0]-1)  \n",
    "        print('n_traj :',pf_samples.shape[0])\n",
    "        n=pf_samples.shape[0]\n",
    "        # var_G_E_X confidence interval estimation \n",
    "        v=var_G_E_X\n",
    "        v_k=np.var((pf_samples-pf_tot_mean)**2)*n/(n-1)**2\n",
    "        var_G_E_X_inf=v-3*np.sqrt(v_k)\n",
    "        var_G_E_X_sup=v+3*np.sqrt(v_k)\n",
    "        \n",
    "        \n",
    "        # verification that the intersection of the two confidence intervals is null \n",
    "        if not((var_G_E_X_inf < var_X_E_G_inf < var_G_E_X_sup) or (var_G_E_X_inf < var_X_E_G_sup < var_G_E_X_sup)):\n",
    "            cond= True\n",
    "            \n",
    "    # verification of the 1st stopping criterion (COV_red)\n",
    "    var_target=(cov_max*pf)**2\n",
    "    if (var_G_E_X_sup+ var_X_E_G_sup < var_target):\n",
    "        print('COV_red = ',np.sqrt(var_G_E_X+ var_X_E_G)/pf)\n",
    "        end_learning=True\n",
    "\n",
    "    return end_learning,var_X_E_G, var_G_E_X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code : Estimation de $V_{tot}$ et $\\hat{P}_f^t$ par Bootstrap "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_variance_bootstrap(gp_g,distribution,IS_sample_size,pf,cov_max,IS_sample,weights_IS_tot,f_aux):\n",
    "    stochastic_dim=gp_g.X.shape[1]\n",
    "    Omega=np.array([[-8.,8.]*stochastic_dim]).reshape(stochastic_dim,2)\n",
    "\n",
    "    eig, vect_star, nodes, weights, M = gp_g.K_eig_noncond_randuniform(nb_vect=1100, Omega=Omega,n_start=2000,eps=1e-8)\n",
    "    phi = gp_g.eval_phi_KL( weights, nodes, eig, vect_star, M, IS_sample)\n",
    "    \n",
    "    # number of realizations of MC and GP \n",
    "    N_mc_i=200 \n",
    "    N_mc_max=5000 \n",
    "    n_traj=1\n",
    "    pf_list=list()\n",
    "    var_target=(cov_max*pf)**2\n",
    "    var_tot_inf=var_target-1\n",
    "    var_tot_sup=var_target+1\n",
    "    N_mc=N_mc_i\n",
    "    traj_ind=np.arange(IS_sample_size)\n",
    "\n",
    "    \n",
    "    while not( not(var_tot_inf<var_target<var_tot_sup) or N_mc>N_mc_max) :\n",
    "        n_traj=N_mc_i\n",
    "        xi_traj=np.random.normal(0,1,(M,n_traj))\n",
    "        traj_i,phi=gp_g.eval_KL_expansion_trajectories_phi(weights=weights,nodes=nodes,eig=eig,vect=vect_star, M=M, x_new=IS_sample,xi_traj=xi_traj,phi=phi) \n",
    "        for i in np.arange(N_mc_i):\n",
    "            ind_boot=np.random.choice(traj_ind,size=IS_sample_size,replace=True)\n",
    "            traj=traj_i.T[ind_boot,i]\n",
    "            weights_IS=weights_IS_tot[ind_boot]\n",
    "            indicatrice = np.zeros((IS_sample_size,n_traj))\n",
    "            ind = traj <=0.0\n",
    "            indicatrice[ind] = 1.0\n",
    "            indicatrice=indicatrice*weights_IS\n",
    "            Pf = indicatrice.sum(axis=0)/IS_sample_size\n",
    "            pf_list.append(Pf)\n",
    "        \n",
    "        pf=np.array(pf_list).flatten()\n",
    "        pf_tot_mean=np.mean(pf)\n",
    "        var_tot=((pf-pf_tot_mean)**2).sum()/(pf.shape[0]-1)\n",
    "        # total variance confidence interval computation\n",
    "        n=pf.shape[0]\n",
    "        v=var_tot\n",
    "        v_k=np.var((pf-pf_tot_mean)**2)*n/(n-1)**2\n",
    "        var_tot_inf=v-3*np.sqrt(v_k)\n",
    "        var_tot_sup=v+3*np.sqrt(v_k)\n",
    "        N_mc=N_mc+N_mc_i \n",
    "\n",
    "    \n",
    "    return var_tot,var_tot_inf,var_tot_sup,pf_tot_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_4b(X):\n",
    "    U1=3+0.1*((X[0]-X[1])**2)-((X[0]+X[1])/np.sqrt(2))\n",
    "    U2=3+0.1*((X[0]-X[1])**2)+((X[0]+X[1])/np.sqrt(2))\n",
    "     \n",
    "    U3=X[0]-X[1]+(6./np.sqrt(2))\n",
    "    U4=X[1]-X[0]+(6./np.sqrt(2))\n",
    "#    U5=X[:,0]-X[:,1]+(7/np.sqrt(2))*np.ones(n).reshape(n,1)\n",
    "#    U6=X[:,1]-X[:,0]+(7/np.sqrt(2))*np.ones(n).reshape(n,1)\n",
    "    Z=np.minimum(U1,U2)\n",
    "    Z=np.minimum(Z,np.array(U3))\n",
    "    Z=np.minimum(Z,np.array(U4))\n",
    "#    Z=np.minimum(Z,np.array(U5))\n",
    "#    Z=np.minimum(Z,np.array(U6))\n",
    "    return Z +1.5\n",
    "stochastic_dim = 2\n",
    "distribution_4b=ot.Normal(ot.Point([0]*stochastic_dim), ot.CovarianceMatrix(stochastic_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########  choix cas test + distribution et param algo \n",
    "fct_test= f_4b\n",
    "distrib_test=distribution_4b\n",
    "initial_doe_size=16\n",
    "initial_MC_sample_size=10000\n",
    "cov_max=0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "('Evalulation of the perfomance function on the initial doe of size ', 16)\n",
      "--------------------------------------\n",
      "4.600553583340887\n",
      "3.0605503072076674\n",
      "2.751161753000482\n",
      "3.825448710035188\n",
      "4.128991614244523\n",
      "4.173599874735466\n",
      "3.3942990188114295\n",
      "2.89011217334915\n",
      "3.8668445505702076\n",
      "4.070479588506203\n",
      "4.049800215120483\n",
      "4.01047638687291\n",
      "3.8575737443092875\n",
      "2.7676458127265793\n",
      "2.8942634844567414\n",
      "4.416545016676093\n",
      "--------------------------------------\n",
      "Saving the initial DoE \n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "('iteration number ', 0)\n",
      "('Pf = ', 0.0)\n",
      "no failure point in MC\n",
      "UPDATE OF AUXILIARY DENSITY\n",
      "2.8190456505784978\n",
      "2.7756161907083494\n",
      "NEW IS INITIAL POP\n",
      "Enrichment of the GP\n",
      "('Number of learning points = ', 4L)\n",
      "0.0914372542878068\n",
      "0.4038469743476343\n",
      "-0.1979259506891351\n",
      "0.31900777240641265\n",
      "--------------------------------------\n",
      "('iteration number ', 1)\n",
      "('Pf = ', 3.408852905672263e-05)\n",
      "('n_traj :', 4000L)\n",
      "('cov_IS = ', 1.0558085336609038)\n",
      "('cov_GP = ', 21.584919746890204)\n",
      "higher variance due to : GP\n",
      "('x min =', array([-1.70544933, -4.82369938]))\n",
      "('val min=', 1.0345588347113481)\n",
      "--------------------------------------\n",
      "('iteration number ', 2)\n",
      "('Pf = ', 3.408852905672263e-05)\n",
      "('n_traj :', 4000L)\n",
      "('cov_IS = ', 1.055132253235918)\n",
      "('cov_GP = ', 21.045664660175383)\n",
      "higher variance due to : GP\n",
      "('x min =', array([-5.09750216,  0.24029801]))\n",
      "('val min=', 0.9917872541715436)\n",
      "--------------------------------------\n",
      "('iteration number ', 3)\n",
      "('Pf = ', 3.408852905672263e-05)\n",
      "('n_traj :', 4000L)\n",
      "('cov_IS = ', 1.086149954819701)\n",
      "('cov_GP = ', 24.028327179336195)\n",
      "higher variance due to : GP\n",
      "('x min =', array([-3.88104687,  1.98408195]))\n",
      "('val min=', 1.0642614491067772)\n",
      "--------------------------------------\n",
      "('iteration number ', 4)\n",
      "('Pf = ', 3.408852905672263e-05)\n",
      "('n_traj :', 4000L)\n",
      "('cov_IS = ', 1.247515600114189)\n",
      "('cov_GP = ', 25.53243913433087)\n",
      "higher variance due to : GP\n",
      "('x min =', array([3.8641771 , 2.97889118]))\n",
      "('val min=', 1.1268306800263388)\n",
      "--------------------------------------\n",
      "('iteration number ', 5)\n",
      "('Pf = ', 3.408852905672263e-05)\n",
      "('n_traj :', 4000L)\n",
      "('cov_IS = ', 1.364875213852927)\n",
      "('cov_GP = ', 26.870515914499137)\n",
      "higher variance due to : GP\n",
      "('x min =', array([4.60693549, 1.35764189]))\n",
      "('val min=', 1.2170586094265428)\n",
      "--------------------------------------\n",
      "('iteration number ', 6)\n",
      "('Pf = ', 3.408852905672263e-05)\n",
      "('n_traj :', 4000L)\n",
      "('cov_IS = ', 1.2980470785232867)\n",
      "('cov_GP = ', 26.508180615436395)\n",
      "higher variance due to : GP\n",
      "('x min =', array([0.95488771, 4.39764136]))\n",
      "('val min=', 1.218211013856265)\n",
      "--------------------------------------\n",
      "('iteration number ', 7)\n",
      "('Pf = ', 3.408852905672263e-05)\n",
      "('n_traj :', 4000L)\n",
      "('cov_IS = ', 1.260087336923848)\n",
      "('cov_GP = ', 22.438812403517765)\n",
      "higher variance due to : GP\n",
      "('x min =', array([-4.23119757, -2.02786555]))\n",
      "('val min=', 1.0792264066277621)\n",
      "--------------------------------------\n",
      "('iteration number ', 8)\n",
      "('Pf = ', 5.239357155350358e-05)\n",
      "('n_traj :', 4000L)\n",
      "('cov_IS = ', 0.827253164000629)\n",
      "('cov_GP = ', 14.372017156882796)\n",
      "higher variance due to : GP\n",
      "('x min =', array([-2.63917345,  3.03690529]))\n",
      "('val min=', 1.058045344377303)\n",
      "--------------------------------------\n",
      "('iteration number ', 9)\n",
      "('Pf = ', 7.473373646561303e-05)\n",
      "('n_traj :', 4000L)\n",
      "('cov_IS = ', 0.6263205668117777)\n",
      "('cov_GP = ', 9.833084062338942)\n",
      "higher variance due to : GP\n",
      "('x min =', array([-2.87981434, -3.83167215]))\n",
      "('val min=', 0.9472752922872483)\n",
      "--------------------------------------\n",
      "('iteration number ', 10)\n",
      "('Pf = ', 7.473373646561303e-05)\n",
      "('n_traj :', 4000L)\n",
      "('cov_IS = ', 0.6330467119989663)\n",
      "('cov_GP = ', 10.28328114417708)\n",
      "higher variance due to : GP\n",
      "('x min =', array([2.57890358, 3.45056933]))\n",
      "('val min=', 0.9048292969023624)\n",
      "--------------------------------------\n",
      "('iteration number ', 11)\n",
      "('Pf = ', 7.473373646561303e-05)\n",
      "('n_traj :', 4000L)\n",
      "('cov_IS = ', 0.6197488134520611)\n",
      "('cov_GP = ', 10.654676349794473)\n",
      "higher variance due to : GP\n",
      "('x min =', array([-1.5290011 ,  3.90323546]))\n",
      "('val min=', 0.8167556721321181)\n",
      "--------------------------------------\n",
      "('iteration number ', 12)\n",
      "('Pf = ', 5.239357155350358e-05)\n",
      "('n_traj :', 4000L)\n",
      "('cov_IS = ', 0.845073237461835)\n",
      "('cov_GP = ', 13.136379174677955)\n",
      "higher variance due to : GP\n",
      "('x min =', array([ 3.70302018, -1.28285341]))\n",
      "('val min=', 0.8093497076168382)\n",
      "--------------------------------------\n",
      "('iteration number ', 13)\n",
      "('Pf = ', 6.69956015774406e-05)\n",
      "('n_traj :', 4000L)\n",
      "('cov_IS = ', 0.590815827435498)\n",
      "('cov_GP = ', 10.310122066488402)\n",
      "higher variance due to : GP\n",
      "('x min =', array([-0.65853052, -3.64933942]))\n",
      "('val min=', 0.7957245214029044)\n",
      "--------------------------------------\n",
      "('iteration number ', 14)\n",
      "('Pf = ', 6.69956015774406e-05)\n",
      "('n_traj :', 4000L)\n",
      "('cov_IS = ', 0.5642914022418883)\n",
      "('cov_GP = ', 6.2652205518587945)\n",
      "higher variance due to : GP\n",
      "('x min =', array([-3.70428373,  0.63058289]))\n",
      "('val min=', 0.7805024489762724)\n",
      "--------------------------------------\n",
      "('iteration number ', 15)\n",
      "('Pf = ', 4.8690559080659646e-05)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-8a7377d6e986>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mPf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mDoE\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mgp_g\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mIS_sample_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mf_aux\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcov_tot\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcov_tot_inf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcov_tot_sup\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mVb_AGP_IS\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfct_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdistrib_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0minitial_MC_sample_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minitial_MC_sample_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0minitial_doe_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minitial_doe_size\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[0miter_max\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m200\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcov_max\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcov_max\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcorr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'matern52'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-2-bd7fc7cc3f17>\u001b[0m in \u001b[0;36mVb_AGP_IS\u001b[1;34m(performance_function, rv_distribution, initial_MC_sample_size, initial_doe_size, iter_max, cov_max, hot_start, path, max_IS_size, corr)\u001b[0m\n\u001b[0;32m    128\u001b[0m                 \u001b[0mvar_X_E_G\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvar_G_E_X\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mend_learning\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvar_X_E_G_inf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvar_X_E_G_sup\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvar_G_E_X_inf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvar_G_E_X_sup\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mresults_sensitivity\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 130\u001b[1;33m                 \u001b[0mresults_sensitivity\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msensitivity_analysis_IS\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgp_g\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mrv_distribution\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mMC_sample_IS\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mweights_IS\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mPf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcov_max\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mf_aux\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlearn_Faux\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mgp_new\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgp_new\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    131\u001b[0m                 \u001b[0mend_learning\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvar_X_E_G\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvar_G_E_X\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mresults_sensitivity\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-ef73a3be5296>\u001b[0m in \u001b[0;36msensitivity_analysis_IS\u001b[1;34m(gp_g, distribution, IS_sample, weights_IS, pf, cov_max, f_aux, learn_Faux, gp_new, path)\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mOmega\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m8.\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m8.\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mstochastic_dim\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstochastic_dim\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m         \u001b[0meig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvect_star\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnodes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mM\u001b[0m \u001b[1;33m=\u001b[0m   \u001b[0mgp_g\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mK_eig_noncond_randuniform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnb_vect\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mOmega\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mOmega\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mn_start\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2000\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0meps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1e-8\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m         \u001b[0meig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'eig.pk'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0mvect_star\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mM\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'vect_star.pk'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\mmenz\\Desktop\\notebook_vb_agp\\gaussian_process.pyc\u001b[0m in \u001b[0;36mK_eig_noncond_randuniform\u001b[1;34m(self, Omega, nb_vect, normalize_Omega, n_start, eps)\u001b[0m\n\u001b[0;32m   1191\u001b[0m             \u001b[0meig\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0meig_new\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1192\u001b[0m             \u001b[0msvd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTruncatedSVD\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mn_comp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_iter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1193\u001b[1;33m             \u001b[0msvd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mB\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1194\u001b[0m             \u001b[0meig_new\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msvd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msingular_values_\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1195\u001b[0m             \u001b[0mcrit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1.0\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0meig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0meig_new\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\mmenz\\Anaconda2\\lib\\site-packages\\sklearn\\decomposition\\truncated_svd.pyc\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    139\u001b[0m             \u001b[0mReturns\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mtransformer\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m         \"\"\"\n\u001b[1;32m--> 141\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    142\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\mmenz\\Anaconda2\\lib\\site-packages\\sklearn\\decomposition\\truncated_svd.pyc\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    175\u001b[0m             U, Sigma, VT = randomized_svd(X, self.n_components,\n\u001b[0;32m    176\u001b[0m                                           \u001b[0mn_iter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 177\u001b[1;33m                                           random_state=random_state)\n\u001b[0m\u001b[0;32m    178\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    179\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"unknown algorithm %r\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0malgorithm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\mmenz\\Anaconda2\\lib\\site-packages\\sklearn\\utils\\extmath.pyc\u001b[0m in \u001b[0;36mrandomized_svd\u001b[1;34m(M, n_components, n_oversamples, n_iter, power_iteration_normalizer, transpose, flip_sign, random_state)\u001b[0m\n\u001b[0;32m    363\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    364\u001b[0m     Q = randomized_range_finder(M, n_random, n_iter,\n\u001b[1;32m--> 365\u001b[1;33m                                 power_iteration_normalizer, random_state)\n\u001b[0m\u001b[0;32m    366\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    367\u001b[0m     \u001b[1;31m# project M to the (k + p) dimensional space using the basis vectors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\mmenz\\Anaconda2\\lib\\site-packages\\sklearn\\utils\\extmath.pyc\u001b[0m in \u001b[0;36mrandomized_range_finder\u001b[1;34m(A, size, n_iter, power_iteration_normalizer, random_state)\u001b[0m\n\u001b[0;32m    248\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mpower_iteration_normalizer\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'LU'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    249\u001b[0m             \u001b[0mQ\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlinalg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msafe_sparse_dot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mA\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mQ\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpermute_l\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 250\u001b[1;33m             \u001b[0mQ\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlinalg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msafe_sparse_dot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mA\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mQ\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpermute_l\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    251\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mpower_iteration_normalizer\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'QR'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    252\u001b[0m             \u001b[0mQ\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlinalg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mqr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msafe_sparse_dot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mA\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mQ\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'economic'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\mmenz\\Anaconda2\\lib\\site-packages\\sklearn\\utils\\extmath.pyc\u001b[0m in \u001b[0;36msafe_sparse_dot\u001b[1;34m(a, b, dense_output)\u001b[0m\n\u001b[0;32m    171\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    172\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 173\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    174\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    175\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "Pf,DoE,n_iter,gp_g, Y, IS_sample_size,f_aux, cov_tot, cov_tot_inf,cov_tot_sup= Vb_AGP_IS(fct_test,distrib_test,initial_MC_sample_size = initial_MC_sample_size,initial_doe_size = initial_doe_size,  iter_max = 200, cov_max=cov_max,corr='matern52')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
